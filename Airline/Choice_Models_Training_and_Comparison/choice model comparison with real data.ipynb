{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "import training_function_real_data as tp\n",
    "import build_new_data as bnd\n",
    "# import assortment as at\n",
    "import warnings\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the original data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "for column in ['stayDurationMinutes', 'totalPrice','totalTripDurationMinutes', 'dtd', 'nAirlines', 'nFlights','outDepTime_sin', 'outDepTime_cos','outArrTime_sin', 'outArrTime_cos']:\n",
    "  max1 = max(data[column])\n",
    "  min1 = min(data[column])\n",
    "  data[column] = (data[column]-min1)/(max1-min1)\n",
    "\n",
    "total_order_list = list(set(data['orderid']))\n",
    "total_num = len(total_order_list)\n",
    "\n",
    "#Generate the corresponding relationship between order_id and row number\n",
    "total_dic = {}\n",
    "for rows in range(len(data)):\n",
    "    temp_id = data.loc[rows,'orderid']\n",
    "    if temp_id in total_dic:\n",
    "        total_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        total_dic[temp_id] = [rows]\n",
    "\n",
    "order_list_random = np.random.permutation(total_order_list)\n",
    "train_order = order_list_random[:int(0.8*total_num)]\n",
    "valid_order = order_list_random[int(0.8*total_num):int(0.9*total_num)]\n",
    "test_order = order_list_random[int(0.9*total_num):]\n",
    "\n",
    "train_list = []\n",
    "valid_list = []\n",
    "test_list = []\n",
    "for orderid in train_order:\n",
    "    train_list.extend(total_dic[orderid])\n",
    "for orderid in valid_order:\n",
    "    valid_list.extend(total_dic[orderid])\n",
    "for orderid in test_order:\n",
    "    test_list.extend(total_dic[orderid])\n",
    "train_data = data.loc[train_list]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "valid_data = data.loc[valid_list]\n",
    "valid_data = valid_data.reset_index(drop=True)\n",
    "test_data = data.loc[test_list]\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record assortment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dic = {}\n",
    "for rows in range(len(train_data)):\n",
    "    temp_id = train_data.loc[rows,'orderid']\n",
    "    if temp_id in train_dic:\n",
    "        train_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        train_dic[temp_id] = [rows]\n",
    "\n",
    "valid_dic = {}\n",
    "temp_id = -1\n",
    "for rows in range(len(valid_data)):\n",
    "    temp_id = valid_data.loc[rows,'orderid']\n",
    "    if temp_id in valid_dic:\n",
    "        valid_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        valid_dic[temp_id] = [rows]\n",
    "\n",
    "test_dic = {}\n",
    "temp_id = -1\n",
    "for rows in range(len(test_data)):\n",
    "    temp_id = test_data.loc[rows,'orderid']\n",
    "    if temp_id in test_dic:\n",
    "        test_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        test_dic[temp_id] = [rows]\n",
    "np.save('train_dic.npy', train_dic)\n",
    "np.save('valid_dic.npy', valid_dic)\n",
    "np.save('test_dic.npy', test_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "label = train_data['orderlabel']\n",
    "\n",
    "#Convert training data and labels to numpy format\n",
    "del_feature = ['orderid','alternative','orderlabel']\n",
    "features = [i for i in train_data.columns if i not in del_feature]\n",
    "\n",
    "data_np = np.array(train_data[features])\n",
    "data_np_cate = np.zeros(shape = data_np.shape)\n",
    "for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "    if item !=1 :\n",
    "        data_np_cate[:,idx] = data_np[:,idx]\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "    if item != 1:\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "total_train_data = np.array([data_np_cate,data_np])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_ztn.npy',total_train_data)\n",
    "np.save('label_ztn.npy',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 106296, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = valid_data['orderlabel']\n",
    "#Preprocess the new validation set\n",
    "del_feature = ['orderid','alternative','orderlabel']\n",
    "features = [i for i in valid_data.columns if i not in del_feature]\n",
    "data_np = np.array(valid_data[features])\n",
    "data_np_cate = np.zeros(shape = data_np.shape)\n",
    "for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "    if item !=1 :\n",
    "        data_np_cate[:,idx] = data_np[:,idx]\n",
    "        data_np[:,idx] = 1\n",
    "        \n",
    "for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "    if item != 1:\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "total_valid_data = np.array([data_np_cate,data_np])\n",
    "total_valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('valid_data_ztn.npy',total_valid_data)\n",
    "np.save('valid_label_ztn.npy',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = test_data['orderlabel']\n",
    "#Preprocess the new validation set\n",
    "del_feature = ['orderid','alternative','orderlabel']\n",
    "features = [i for i in test_data.columns if i not in del_feature]\n",
    "data_np = np.array(test_data[features])\n",
    "data_np_cate = np.zeros(shape = data_np.shape)\n",
    "for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "    if item !=1 :\n",
    "        data_np_cate[:,idx] = data_np[:,idx]\n",
    "        data_np[:,idx] = 1\n",
    "        \n",
    "for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "    if item != 1:\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "total_test_data = np.array([data_np_cate,data_np])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('test_data_ztn.npy',total_test_data)\n",
    "np.save('test_label_ztn.npy',label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('data_ztn.npy')\n",
    "train_label = np.load('label_ztn.npy')\n",
    "train_dic = np.load('train_dic.npy', allow_pickle=True).item()\n",
    "\n",
    "valid_data = np.load('valid_data_ztn.npy')\n",
    "valid_label = np.load('valid_label_ztn.npy')\n",
    "valid_dic = np.load('valid_dic.npy', allow_pickle=True).item()\n",
    "\n",
    "test_data = np.load('test_data_ztn.npy')\n",
    "test_label = np.load('test_label_ztn.npy')\n",
    "test_dic = np.load('test_dic.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for DeepFM-a\n",
    "data = deepcopy(train_data)\n",
    "train_data_assort = np.array([np.concatenate([data[0],np.zeros((train_data.shape[1], 9)),np.zeros((train_data.shape[1], 2))],axis = 1),np.concatenate([data[1],np.ones((train_data.shape[1], 9)),np.ones((train_data.shape[1], 2))],axis = 1)])\n",
    "for i in train_dic.values():\n",
    "    data_temp = np.zeros((len(i), 9))\n",
    "    data_cate_temp = np.zeros((len(i), 2))\n",
    "    min1 = np.min(data[1,i,5])\n",
    "    max1 = np.max(data[1,i,5])\n",
    "    mean1 = np.mean(data[1,i,5])\n",
    "    min2 = np.min(data[1,i,6])\n",
    "    max2 = np.max(data[1,i,6])\n",
    "    mean2 = np.mean(data[1,i,6])\n",
    "    min3 = np.min(data[1,i,7])\n",
    "    max3 = np.max(data[1,i,7])\n",
    "    mean3 = np.mean(data[1,i,7])\n",
    "    data_temp[:,0] = min1\n",
    "    data_temp[:,1] = max1\n",
    "    data_temp[:,2] = mean1\n",
    "    data_temp[:,3] = min2\n",
    "    data_temp[:,4] = max2\n",
    "    data_temp[:,5] = mean2\n",
    "    data_temp[:,6] = min3\n",
    "    data_temp[:,7] = max3\n",
    "    data_temp[:,8] = mean3 \n",
    "    for idx,j in enumerate(i):\n",
    "        if data[1,j,6] == np.min(data[1,i,6]):\n",
    "            data_cate_temp[idx,0] = 1\n",
    "        if data[1,j,6] == np.max(data[1,i,6]):\n",
    "            data_cate_temp[idx,1] = 1\n",
    "    train_data_assort[0,i,-2:] = data_cate_temp\n",
    "    train_data_assort[1,i,17:26] = data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(valid_data)\n",
    "valid_data_assort = np.array([np.concatenate([data[0],np.zeros((data.shape[1], 9)),np.zeros((data.shape[1], 2))],axis = 1),np.concatenate([data[1],np.ones((data.shape[1], 9)),np.ones((data.shape[1], 2))],axis = 1)])\n",
    "for i in valid_dic.values():\n",
    "    data_temp = np.zeros((len(i), 9))\n",
    "    data_cate_temp = np.zeros((len(i), 2))\n",
    "    min1 = np.min(data[1,i,5])\n",
    "    max1 = np.max(data[1,i,5])\n",
    "    mean1 = np.mean(data[1,i,5])\n",
    "    min2 = np.min(data[1,i,6])\n",
    "    max2 = np.max(data[1,i,6])\n",
    "    mean2 = np.mean(data[1,i,6])\n",
    "    min3 = np.min(data[1,i,7])\n",
    "    max3 = np.max(data[1,i,7])\n",
    "    mean3 = np.mean(data[1,i,7])\n",
    "    data_temp[:,0] = min1\n",
    "    data_temp[:,1] = max1\n",
    "    data_temp[:,2] = mean1\n",
    "    data_temp[:,3] = min2\n",
    "    data_temp[:,4] = max2\n",
    "    data_temp[:,5] = mean2\n",
    "    data_temp[:,6] = min3\n",
    "    data_temp[:,7] = max3\n",
    "    data_temp[:,8] = mean3 \n",
    "    for idx,j in enumerate(i):\n",
    "        if data[1,j,6] == np.min(data[1,i,6]):\n",
    "            data_cate_temp[idx,0] = 1\n",
    "        if data[1,j,6] == np.max(data[1,i,6]):\n",
    "            data_cate_temp[idx,1] = 1\n",
    "    valid_data_assort[0,i,-2:] = data_cate_temp\n",
    "    valid_data_assort[1,i,17:26] = data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(test_data)\n",
    "test_data_assort = np.array([np.concatenate([data[0],np.zeros((data.shape[1], 9)),np.zeros((data.shape[1], 2))],axis = 1),np.concatenate([data[1],np.ones((data.shape[1], 9)),np.ones((data.shape[1], 2))],axis = 1)])\n",
    "for i in test_dic.values():\n",
    "    data_temp = np.zeros((len(i), 9))\n",
    "    data_cate_temp = np.zeros((len(i), 2))\n",
    "    min1 = np.min(data[1,i,5])\n",
    "    max1 = np.max(data[1,i,5])\n",
    "    mean1 = np.mean(data[1,i,5])\n",
    "    min2 = np.min(data[1,i,6])\n",
    "    max2 = np.max(data[1,i,6])\n",
    "    mean2 = np.mean(data[1,i,6])\n",
    "    min3 = np.min(data[1,i,7])\n",
    "    max3 = np.max(data[1,i,7])\n",
    "    mean3 = np.mean(data[1,i,7])\n",
    "    data_temp[:,0] = min1\n",
    "    data_temp[:,1] = max1\n",
    "    data_temp[:,2] = mean1\n",
    "    data_temp[:,3] = min2\n",
    "    data_temp[:,4] = max2\n",
    "    data_temp[:,5] = mean2\n",
    "    data_temp[:,6] = min3\n",
    "    data_temp[:,7] = max3\n",
    "    data_temp[:,8] = mean3 \n",
    "    for idx,j in enumerate(i):\n",
    "        if data[1,j,6] == np.min(data[1,i,6]):\n",
    "            data_cate_temp[idx,0] = 1\n",
    "        if data[1,j,6] == np.max(data[1,i,6]):\n",
    "            data_cate_temp[idx,1] = 1\n",
    "    test_data_assort[0,i,-2:] = data_cate_temp\n",
    "    test_data_assort[1,i,17:26] = data_temp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MNL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Train the MNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_MNL = np.concatenate((train_data,valid_data),axis = 1)\n",
    "train_label_MNL = np.concatenate((train_label,valid_label),axis = 0)\n",
    "train_dic_MNL = deepcopy(train_dic)\n",
    "for i in valid_dic.keys():\n",
    "    if i in train_dic.keys():\n",
    "        print('error')\n",
    "    else:\n",
    "        train_dic_MNL[i] = list(np.array(valid_dic[i]) + train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_model_ztn\\\\MNL_parameters_ztn.pt\"\n",
    "model_MNL = tp.train_data(train_data_MNL,train_label_MNL, train_dic_MNL, NUM_EPOCHS = 100,BATCH_SIZE = 32,path = PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MNL = torch.load(\"trained_model_ztn\\\\MNL_parameters_ztn.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(test_data[0]).to(torch.long)\n",
    "weight = torch.from_numpy(test_data[1]).to(torch.float)\n",
    "utility_MNL = model_MNL([X,weight])\n",
    "utility_MNL = utility_MNL.detach().numpy()\n",
    "for item in test_dic:\n",
    "    temp_lst = test_dic[item]\n",
    "    temp_sum = np.sum(np.exp(np.array(utility_MNL[test_dic[item]])))\n",
    "    for i in temp_lst:\n",
    "        utility_MNL[i] = np.exp(utility_MNL[i]) /temp_sum\n",
    "\n",
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    #print(np.sum(utility_MNL[test_dic[item]]))\n",
    "    rmse.append(np.sum((utility_MNL[test_dic[item]]-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-utility_MNL[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    # print(test_label[test_dic[item]])\n",
    "    # print(utility_MNL[test_dic[item]])\n",
    "    # print(np.sum((utility_MNL[test_dic[item]]-test_label[test_dic[item]])**2))\n",
    "    # input()\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)\n",
    "print(total)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "logloss = 0\n",
    "for i in range(len(utility_MNL)):\n",
    "    if test_label[i] == 1:\n",
    "        logloss += -test_label[i] * (np.log(utility_MNL[i]*5000)-np.log(5000))\n",
    "    else:\n",
    "        logloss += -(1-test_label[i]) * (np.log((1-utility_MNL[i])*5000)-np.log(5000))\n",
    "logloss = logloss/len(utility_MNL)\n",
    "print(logloss)\n",
    "print(roc_auc_score(test_label, utility_MNL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 MMNL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_MNL = np.concatenate((train_data,valid_data),axis = 1)\n",
    "train_label_MNL = np.concatenate((train_label,valid_label),axis = 0)\n",
    "train_dic_MNL = deepcopy(train_dic)\n",
    "for i in valid_dic.keys():\n",
    "    if i in train_dic.keys():\n",
    "        print('error')\n",
    "    else:\n",
    "        train_dic_MNL[i] = list(np.array(valid_dic[i]) + train_data.shape[1])\n",
    "\n",
    "#PATH = \"trained_model_ztn\\\\MMNL_parameters_ztn.pt\"\n",
    "model_MMNL_list,alpha = tp.CG_algo(train_data_MNL,train_label_MNL, train_dic_MNL, NUM_EPOCHS = 100,BATCH_SIZE = 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Predictin results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MMNL_list = np.load(\"trained_model_ztn\\\\MMNL.npy\",allow_pickle=True)\n",
    "alpha = np.load(\"trained_model_ztn\\\\alpha.npy\")\n",
    "\n",
    "pro_temp = []\n",
    "for model_MNL in model_MMNL_list:\n",
    "    X = torch.from_numpy(test_data[0]).to(torch.long)\n",
    "    weight = torch.from_numpy(test_data[1]).to(torch.float)\n",
    "    utility_MNL = model_MNL([X,weight])\n",
    "    utility_MNL = utility_MNL.detach().numpy()\n",
    "    for item in test_dic:\n",
    "        temp_lst = test_dic[item]\n",
    "        temp_sum = np.sum(np.exp(np.array(utility_MNL[test_dic[item]]))) \n",
    "        for i in temp_lst:\n",
    "            utility_MNL[i] = np.exp(utility_MNL[i]) /temp_sum\n",
    "    pro_temp.append(utility_MNL)\n",
    "pro = 0\n",
    "for i in range(len(pro_temp)):\n",
    "    pro = alpha[i]*np.array(pro_temp[i]) + pro\n",
    "\n",
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    rmse.append(np.sum((pro[test_dic[item]]-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-pro[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 DeepFM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Epoch: 01 ----------\n",
      "loss 1.17276,acc 0.19798\n",
      "---------------- Epoch: 02 ----------\n",
      "loss 0.992891,acc 0.190024\n",
      "---------------- Epoch: 03 ----------\n",
      "loss 1.00676,acc 0.19951\n",
      "---------------- Epoch: 04 ----------\n",
      "loss 0.955415,acc 0.200734\n",
      "---------------- Epoch: 05 ----------\n",
      "loss 0.984403,acc 0.199816\n",
      "---------------- Epoch: 06 ----------\n",
      "loss 0.967779,acc 0.20716\n",
      "---------------- Epoch: 07 ----------\n",
      "loss 0.946068,acc 0.205018\n",
      "---------------- Epoch: 08 ----------\n",
      "loss 0.953947,acc 0.205936\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 09 ----------\n",
      "loss 0.944871,acc 0.2041\n",
      "---------------- Epoch: 10 ----------\n",
      "loss 0.950531,acc 0.20563\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 11 ----------\n",
      "loss 0.942306,acc 0.206854\n",
      "---------------- Epoch: 12 ----------\n",
      "loss 0.959235,acc 0.206242\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 13 ----------\n",
      "loss 0.940268,acc 0.20869\n",
      "---------------- Epoch: 14 ----------\n",
      "loss 0.971805,acc 0.209302\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 15 ----------\n",
      "loss 0.992523,acc 0.205936\n",
      "EarlyStopping counter: 2 out of 10\n",
      "---------------- Epoch: 16 ----------\n",
      "loss 0.939901,acc 0.204712\n",
      "---------------- Epoch: 17 ----------\n",
      "loss 0.943623,acc 0.207466\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 18 ----------\n",
      "loss 0.937709,acc 0.208384\n",
      "---------------- Epoch: 19 ----------\n",
      "loss 0.939667,acc 0.20716\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 20 ----------\n",
      "loss 0.936678,acc 0.20563\n",
      "---------------- Epoch: 21 ----------\n",
      "loss 0.937514,acc 0.207466\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 22 ----------\n",
      "loss 0.934715,acc 0.209608\n",
      "---------------- Epoch: 23 ----------\n",
      "loss 0.935642,acc 0.206548\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 24 ----------\n",
      "loss 0.938564,acc 0.20716\n",
      "EarlyStopping counter: 2 out of 10\n",
      "---------------- Epoch: 25 ----------\n",
      "loss 0.93884,acc 0.204406\n",
      "EarlyStopping counter: 3 out of 10\n",
      "---------------- Epoch: 26 ----------\n",
      "loss 0.938803,acc 0.202264\n",
      "EarlyStopping counter: 4 out of 10\n",
      "---------------- Epoch: 27 ----------\n",
      "loss 0.9333,acc 0.206548\n",
      "---------------- Epoch: 28 ----------\n",
      "loss 0.937848,acc 0.206548\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 29 ----------\n",
      "loss 0.939754,acc 0.205324\n",
      "EarlyStopping counter: 2 out of 10\n",
      "---------------- Epoch: 30 ----------\n",
      "loss 0.933822,acc 0.203794\n",
      "EarlyStopping counter: 3 out of 10\n",
      "---------------- Epoch: 31 ----------\n",
      "loss 0.934644,acc 0.2041\n",
      "EarlyStopping counter: 4 out of 10\n",
      "---------------- Epoch: 32 ----------\n",
      "loss 0.936681,acc 0.20563\n",
      "EarlyStopping counter: 5 out of 10\n",
      "---------------- Epoch: 33 ----------\n",
      "loss 0.959453,acc 0.206854\n",
      "EarlyStopping counter: 6 out of 10\n",
      "---------------- Epoch: 34 ----------\n",
      "loss 0.933351,acc 0.207466\n",
      "EarlyStopping counter: 7 out of 10\n",
      "---------------- Epoch: 35 ----------\n",
      "loss 0.93425,acc 0.205324\n",
      "EarlyStopping counter: 8 out of 10\n",
      "---------------- Epoch: 36 ----------\n",
      "loss 0.945786,acc 0.20716\n",
      "EarlyStopping counter: 9 out of 10\n",
      "---------------- Epoch: 37 ----------\n",
      "loss 0.94465,acc 0.204712\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_model_ztn\\\\DeepFM_parameters_ztn.pt\"\n",
    "model_DeepFM = tp.train_DeepFM(train_data,train_label,valid_data,valid_label,valid_dic, NUM_EPOCHS = 100,BATCH_SIZE = 256,LR = 0.05,weight1 = 31,path = PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepFM = torch.load(\"trained_model_ztn\\\\DeepFM_parameters_ztn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cat = torch.from_numpy(test_data[0]).to(torch.long).to(device)\n",
    "weight = torch.from_numpy(test_data[1]).to(torch.float).to(device)\n",
    "outputs = model_DeepFM([cat, weight])\n",
    "outputs = outputs.cpu().detach().numpy()\n",
    "pro = np.exp(outputs)/(np.exp(outputs)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45724338381520574\n",
      "RMSE:  0.16464937482856498\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    temp_prob_max = []\n",
    "    temp_prob_max = np.argsort(-pro[test_dic[item]])[0:top_n]\n",
    "    denominator += len(test_dic[item])\n",
    "    rmse.append(np.sum((pro[test_dic[item]]/np.sum(pro[test_dic[item]])-test_label[test_dic[item]])**2))\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(count/total)\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 DeepFM-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Epoch: 01 ----------\n",
      "loss 1.05344,acc 0.214504\n",
      "---------------- Epoch: 02 ----------\n",
      "loss 1.00086,acc 0.214198\n",
      "---------------- Epoch: 03 ----------\n",
      "loss 1.3856,acc 0.212362\n",
      "---------------- Epoch: 04 ----------\n",
      "loss 0.950384,acc 0.212668\n",
      "---------------- Epoch: 05 ----------\n",
      "loss 1.1036,acc 0.215116\n",
      "---------------- Epoch: 06 ----------\n",
      "loss 1.09849,acc 0.213586\n",
      "---------------- Epoch: 07 ----------\n",
      "loss 1.40586,acc 0.205018\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 08 ----------\n",
      "loss 1.00352,acc 0.214198\n",
      "---------------- Epoch: 09 ----------\n",
      "loss 0.923876,acc 0.211444\n",
      "---------------- Epoch: 10 ----------\n",
      "loss 1.00327,acc 0.21175\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 11 ----------\n",
      "loss 0.970573,acc 0.211138\n",
      "EarlyStopping counter: 2 out of 10\n",
      "---------------- Epoch: 12 ----------\n",
      "loss 1.0309,acc 0.21481\n",
      "EarlyStopping counter: 3 out of 10\n",
      "---------------- Epoch: 13 ----------\n",
      "loss 1.85013,acc 0.2041\n",
      "EarlyStopping counter: 4 out of 10\n",
      "---------------- Epoch: 14 ----------\n",
      "loss 0.940444,acc 0.211444\n",
      "EarlyStopping counter: 5 out of 10\n",
      "---------------- Epoch: 15 ----------\n",
      "loss 1.1025,acc 0.210832\n",
      "EarlyStopping counter: 6 out of 10\n",
      "---------------- Epoch: 16 ----------\n",
      "loss 0.919325,acc 0.214504\n",
      "---------------- Epoch: 17 ----------\n",
      "loss 0.988355,acc 0.212362\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 18 ----------\n",
      "loss 0.961067,acc 0.212362\n",
      "EarlyStopping counter: 2 out of 10\n",
      "---------------- Epoch: 19 ----------\n",
      "loss 0.947482,acc 0.213586\n",
      "EarlyStopping counter: 3 out of 10\n",
      "---------------- Epoch: 20 ----------\n",
      "loss 0.952875,acc 0.211444\n",
      "EarlyStopping counter: 4 out of 10\n",
      "---------------- Epoch: 21 ----------\n",
      "loss 0.970565,acc 0.21328\n",
      "EarlyStopping counter: 5 out of 10\n",
      "---------------- Epoch: 22 ----------\n",
      "loss 0.963187,acc 0.21481\n",
      "EarlyStopping counter: 6 out of 10\n",
      "---------------- Epoch: 23 ----------\n",
      "loss 0.922536,acc 0.212362\n",
      "EarlyStopping counter: 7 out of 10\n",
      "---------------- Epoch: 24 ----------\n",
      "loss 0.938253,acc 0.21328\n",
      "EarlyStopping counter: 8 out of 10\n",
      "---------------- Epoch: 25 ----------\n",
      "loss 0.933978,acc 0.212974\n",
      "EarlyStopping counter: 9 out of 10\n",
      "---------------- Epoch: 26 ----------\n",
      "loss 1.06958,acc 0.215422\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_model_ztn\\\\DeepFM-a_parameters_ztn.pt\"\n",
    "model_DeepFM = tp.train_DeepFMa(train_data_assort,train_label,valid_data_assort,valid_label,valid_dic, NUM_EPOCHS = 100,BATCH_SIZE = 256,LR = 0.05,weight1 = 31,path = PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepFM = torch.load(\"trained_model_ztn\\\\DeepFM-a_parameters_ztn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cat = torch.from_numpy(test_data_assort[0]).to(torch.long).to(device)\n",
    "weight = torch.from_numpy(test_data_assort[1]).to(torch.float).to(device)\n",
    "outputs = model_DeepFM([cat, weight])\n",
    "outputs = outputs.cpu().detach().numpy()\n",
    "pro = np.exp(outputs)/(np.exp(outputs)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6100657794095151\n",
      "RMSE:  0.16467736989501122\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "top_n = 5\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    temp_prob_max = []\n",
    "    temp_prob_max = np.argsort(-pro[test_dic[item]])[0:top_n]\n",
    "    denominator += len(test_dic[item])\n",
    "    rmse.append(np.sum((pro[test_dic[item]]/np.sum(pro[test_dic[item]])-test_label[test_dic[item]])**2))\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(count/total)\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Exponential Choice Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_exp = np.concatenate((train_data,valid_data),axis = 1)\n",
    "train_label_exp = np.concatenate((train_label,valid_label),axis = 0)\n",
    "train_dic_exp = deepcopy(train_dic)\n",
    "for i in valid_dic.keys():\n",
    "    if i in train_dic.keys():\n",
    "        print('error')\n",
    "    else:\n",
    "        train_dic_exp[i] = list(np.array(valid_dic[i]) + train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Epoch: 01 ----------\n",
      "loss 2.81093\n",
      "---------------- Epoch: 02 ----------\n",
      "loss 2.70854\n",
      "---------------- Epoch: 03 ----------\n",
      "loss 2.73404\n",
      "---------------- Epoch: 04 ----------\n",
      "loss 2.94712\n",
      "---------------- Epoch: 05 ----------\n",
      "loss 2.68275\n",
      "---------------- Epoch: 06 ----------\n",
      "loss 3.18568\n",
      "EarlyStopping counter: 1 out of 10\n",
      "---------------- Epoch: 07 ----------\n",
      "loss 2.70411\n",
      "EarlyStopping counter: 2 out of 10\n",
      "---------------- Epoch: 08 ----------\n",
      "loss 2.75742\n",
      "EarlyStopping counter: 3 out of 10\n",
      "---------------- Epoch: 09 ----------\n",
      "loss 2.77556\n",
      "EarlyStopping counter: 4 out of 10\n",
      "---------------- Epoch: 10 ----------\n",
      "loss 2.73669\n",
      "EarlyStopping counter: 5 out of 10\n",
      "---------------- Epoch: 11 ----------\n",
      "loss 2.71328\n",
      "EarlyStopping counter: 6 out of 10\n",
      "---------------- Epoch: 12 ----------\n",
      "loss 2.996\n",
      "EarlyStopping counter: 7 out of 10\n",
      "---------------- Epoch: 13 ----------\n",
      "loss 2.95275\n",
      "EarlyStopping counter: 8 out of 10\n",
      "---------------- Epoch: 14 ----------\n",
      "loss 2.97581\n",
      "EarlyStopping counter: 9 out of 10\n",
      "---------------- Epoch: 15 ----------\n",
      "loss 2.79618\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_model_ztn\\\\EXP_parameters_ztn.pt\"\n",
    "model_MNL = tp.train_data(train_data_exp,train_label_exp, train_dic_exp, NUM_EPOCHS = 100,BATCH_SIZE = 32,path = PATH, model_type = 'EXP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_model_ztn\\\\EXP_parameters_ztn.pt\"\n",
    "model_exp = torch.load(PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(test_data[0]).to(torch.long)\n",
    "weight = torch.from_numpy(test_data[1]).to(torch.float)\n",
    "utility_exp = model_exp([X,weight])\n",
    "utility_exp = utility_exp.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_exp = np.zeros_like(utility_exp)\n",
    "for item in test_dic:\n",
    "    temp_lst = test_dic[item]\n",
    "    utility_temp = utility_exp[test_dic[item]]\n",
    "    utility_temp = np.sort(utility_temp)\n",
    "    for i in temp_lst:\n",
    "        index = np.where(utility_temp == utility_exp[i])[0][0]\n",
    "        Q1 = np.exp(-np.sum(utility_temp[index:] - utility_temp[index]))/(utility_temp.shape[0]-index)\n",
    "        if index == 0:\n",
    "            pro_exp[i] = Q1\n",
    "        else:\n",
    "            Q2 = 0\n",
    "            for j in range(index):\n",
    "                Q2 += np.exp(-np.sum(utility_temp[j:] - utility_temp[j]))/((utility_temp.shape[0]-j-1)*(utility_temp.shape[0]-j))\n",
    "            pro_exp[i] = Q1-Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.1661375263459456\n",
      "0.40767936362245677\n",
      "6537\n",
      "0.11219007450757641\n",
      "0.839770912228547\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    rmse.append(np.sum((pro_exp[test_dic[item]]-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-pro_exp[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \", np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)\n",
    "print(total)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "logloss = 0\n",
    "for i in range(len(pro_exp)):\n",
    "    if test_label[i] == 1:\n",
    "        logloss += -test_label[i] * (np.log(pro_exp[i]*5000)-np.log(5000))\n",
    "    else:\n",
    "        logloss += -(1-test_label[i]) * (np.log((1-pro_exp[i])*5000)-np.log(5000))\n",
    "logloss = logloss/len(pro_exp)\n",
    "print(logloss)\n",
    "print(roc_auc_score(test_label, pro_exp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dbc828abb6ce728a8b93da7b8ed7a63770a70f4d8c4b8f352b606b709a27647"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
