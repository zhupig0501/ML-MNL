{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from true_model import true_model\n",
    "from copy import deepcopy\n",
    "import MNL_or_EC as tp\n",
    "import LC_MNL as LM\n",
    "import build_new_data as bnd\n",
    "import assortment_process as at\n",
    "import warnings\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from alpermodel_1 import conicmc\n",
    "#from transformer_using_train_dataset_20220801 import trained_model\n",
    "from collections import Counter\n",
    "from  new_transformer_combination import Modifiled_Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the new traing data or testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_size = 30\n",
    "order_num = 10000\n",
    "file_name = \"test\"\n",
    "\n",
    "#c_rate is \\gamma\n",
    "for c_rate in [5,10,20,50,100,200]:\n",
    "    print(c_rate)\n",
    "    next_train_data = bnd.generate_data(order_num,int(choice_size+c_rate))\n",
    "    next_train_data.to_csv(file_name+\"_dataset\\\\\"+str(c_rate)+'_'+str(order_num)+'.csv',index=False)\n",
    "    \n",
    "    #transformer get lables for data\n",
    "    pro = true_model.true_model(next_train_data,order_num)\n",
    "    pro = pro.reshape(order_num,int(choice_size+c_rate))\n",
    "    label = np.zeros(shape = pro.shape)\n",
    "    for i in range(pro.shape[0]):\n",
    "        index = np.argmax(np.random.multinomial(1,pro[i]/np.sum(pro[i])))\n",
    "        label[i,index] = 1\n",
    "    label = label[:,0:choice_size]\n",
    "    pro = pro[:,0:choice_size]\n",
    "\n",
    "    del_feature = ['orderid']\n",
    "    features = [i for i in next_train_data.columns if i not in del_feature]\n",
    "    data_numpy = np.array(next_train_data[features]).reshape(order_num,int(choice_size+c_rate),-1)\n",
    "    np.save(file_name+\"_dataset_numpy\\\\data_\"+str(c_rate)+'_'+str(order_num)+'.npy',data_numpy[:,0:choice_size,:])\n",
    "    np.save(file_name+\"_dataset_numpy\\\\label_\"+str(c_rate)+'_'+str(order_num)+'.npy',label)\n",
    "    np.save(file_name+\"_dataset_numpy\\\\pro_\"+str(c_rate)+'_'+str(order_num)+'.npy',pro)\n",
    "    print(pro[0,:])\n",
    "\n",
    "    ##Transform into the numpy data\n",
    "    del_feature = ['orderid']\n",
    "    features = [i for i in next_train_data.columns if i not in del_feature]\n",
    "    data_np = np.array(next_train_data[features])\n",
    "    data_np_cate = np.zeros(shape = data_np.shape)\n",
    "    for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "        if item !=1 :\n",
    "            data_np_cate[:,idx] = data_np[:,idx]\n",
    "            data_np[:,idx] = 1\n",
    "\n",
    "    scaler = MinMaxScaler().fit(data_np)\n",
    "    data_np = scaler.transform(data_np)\n",
    "    for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "        if item != 1:\n",
    "            data_np[:,idx] = 1\n",
    "\n",
    "    data_np = data_np.reshape(-1,int(choice_size+c_rate),len(features))\n",
    "    data_np_cate = data_np_cate.reshape(-1,int(choice_size+c_rate),len(features))\n",
    "    data = np.array([data_np_cate[:,0:choice_size,:],data_np[:,0:choice_size,:]])\n",
    "\n",
    "    #Incorporate the assortment information for DeemFM-a\n",
    "    data_temp = np.zeros((order_num, choice_size, 9))\n",
    "    data_cate_temp = np.zeros((order_num, choice_size, 2))\n",
    "    for i in range(order_num):\n",
    "        min1 = np.min(data[1,i,:,5])\n",
    "        max1 = np.max(data[1,i,:,5])\n",
    "        mean1 = np.mean(data[1,i,:,5])\n",
    "        min2 = np.min(data[1,i,:,6])\n",
    "        max2 = np.max(data[1,i,:,6])\n",
    "        mean2 = np.mean(data[1,i,:,6])\n",
    "        min3 = np.min(data[1,i,:,7])\n",
    "        max3 = np.max(data[1,i,:,7])\n",
    "        mean3 = np.mean(data[1,i,:,7])\n",
    "        data_temp[i,:,0] = min1\n",
    "        data_temp[i,:,1] = max1\n",
    "        data_temp[i,:,2] = mean1\n",
    "        data_temp[i,:,3] = min2\n",
    "        data_temp[i,:,4] = max2\n",
    "        data_temp[i,:,5] = mean2\n",
    "        data_temp[i,:,6] = min3\n",
    "        data_temp[i,:,7] = max3\n",
    "        data_temp[i,:,8] = mean3 \n",
    "        for j in range(choice_size):\n",
    "            if data[1,i,j,6] == np.min(data[1,i,:,6]):\n",
    "                data_cate_temp[i,j,0] = 1\n",
    "            if data[1,i,j,6] == np.max(data[1,i,:,6]):\n",
    "                data_cate_temp[i,j,1] = 1\n",
    "    data_with_assort_info = np.array([np.concatenate([data[0],np.zeros((order_num, choice_size, 9)),data_cate_temp],axis = 2),np.concatenate([data[1],data_temp,np.ones((order_num,choice_size,2))],axis = 2)])\n",
    "\n",
    "    np.save(file_name+'_dataset_numpy_normalised\\\\data_'+str(c_rate)+'_'+str(order_num)+'.npy',data)\n",
    "    np.save(file_name+'_dataset_numpy_normalised\\\\label_'+str(c_rate)+'_'+str(order_num)+'.npy',label)\n",
    "    np.save(file_name+'_dataset_numpy_normalised\\\\data_with_assort_info_'+str(c_rate)+'_'+str(order_num )+'.npy',data_with_assort_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the decision-test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "for c_rate in [200]:\n",
    "    print(c_rate)\n",
    "    next_valid_data = bnd.generate_data(1000,100+c_rate)\n",
    "    del_feature = ['orderid','alternative','orderlabel']\n",
    "    features = [i for i in next_valid_data.columns if i not in del_feature]\n",
    "    data_np = np.array(next_valid_data[features])\n",
    "    scaler = MinMaxScaler().fit(data_np)\n",
    "    data_np_cate = np.zeros(shape = data_np.shape)\n",
    "    for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "        if item !=1 :\n",
    "            data_np_cate[:,idx] = data_np[:,idx]\n",
    "            data_np[:,idx] = 1\n",
    "    data_np = scaler.transform(data_np)\n",
    "    for idx,item in enumerate([11, 7, 97, 63, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1]):\n",
    "        if item != 1:\n",
    "            data_np[:,idx] = 1\n",
    "\n",
    "    data_np = data_np.reshape(-1,100+c_rate,len(features))\n",
    "    data_np_cate = data_np_cate.reshape(-1,100+c_rate,len(features))\n",
    "    valid_data = np.array([data_np_cate,data_np])\n",
    "    next_valid_data.to_csv('new_products\\\\next_valid_data_'+str(c_rate)+'.csv',index=False)\n",
    "    np.save('new_products\\\\valid_data_'+str(c_rate)+'.npy',valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MNL-Based Choice Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Training the MNL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_models\\\\MNL_parameters_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_MNL = tp.train_data(data,label,NUM_EPOCHS = 100,BATCH_SIZE = 32,LR = 0.01,path = PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MNL Top1-acc: 0.1348\n",
      " MNL Top3-acc: 0.309\n",
      " Averaged raito of MNL: 4.79226342945531\n",
      "RMSE of MNL: 0.03939\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_models\\\\MNL_parameters_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_MNL = torch.load(PATH)\n",
    "\n",
    "X = torch.from_numpy(test_data[0][:,:,feature_column]).to(torch.long)\n",
    "weight = torch.from_numpy(test_data[1][:,:,feature_column]).to(torch.float)\n",
    "utility_MNL,u0 = model_MNL([X,weight])\n",
    "utility_MNL = utility_MNL.detach().numpy()\n",
    "u0 = float(u0[0])\n",
    "pro_MNL = np.zeros(shape = utility_MNL.shape)\n",
    "for i in range(pro_MNL.shape[0]):\n",
    "    pro_MNL[i] = np.exp(utility_MNL[i])/(np.sum(np.exp(utility_MNL[i]))+np.exp(u0))\n",
    "\n",
    "ratio = 0\n",
    "high_low_ratio = []\n",
    "for i in range(pro_MNL.shape[0]):\n",
    "    ratio = ratio + (np.max(pro_MNL[i])- np.min(pro_MNL[i]))/np.mean(pro_MNL[i])\n",
    "    #index_array = np.argsort(-test_price[i])\n",
    "    #high_low_ratio.append(pro_MNL[i,index_array[1]]/pro_MNL[i,index_array[0]])\n",
    "\n",
    "rmse = []\n",
    "count_number1 = 0\n",
    "count_number3 = 0\n",
    "for i in range(pro_MNL.shape[0]):\n",
    "    rmse.append(np.sum((pro_MNL[i] - pro[i,0:30])**2) + (np.sum(pro_MNL[i]) - np.sum(pro[i,0:30]))**2)\n",
    "    pro_index = np.argsort(-pro_MNL[i])\n",
    "    if np.sum(test_label[i,:]) > 0:\n",
    "        if np.argmax(test_label[i,:]) in list(pro_index[0:1]):\n",
    "            count_number1 += 1\n",
    "        if np.argmax(test_label[i,:]) in list(pro_index[0:3]):\n",
    "            count_number3 += 1\n",
    "print(\" MNL Top1-acc: {:.4}\".format(count_number1/np.sum(test_label)))\n",
    "print(\" MNL Top3-acc: {:.4}\".format(count_number3/np.sum(test_label)))\n",
    "print(\" Averaged raito of MNL: {}\".format(ratio/10000))\n",
    "print(\"RMSE of MNL: {:.4}\".format(np.sqrt(sum(rmse)/(10000*31))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Assotments Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [14:52<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit: 77.52122141846797\n",
      "12.0 29.937 30.0\n",
      "50:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [23:22<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profit: 72.64559436395216\n",
      "12.0 49.663 50.0\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_models\\\\MNL_parameters_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_MNL = torch.load(PATH)\n",
    "\n",
    "X = torch.from_numpy(valid_data[0]).to(torch.long)\n",
    "weight = torch.from_numpy(valid_data[1]).to(torch.float)\n",
    "utility_MNL,u0 = model_MNL([X,weight])\n",
    "utility_MNL = utility_MNL.detach().numpy()\n",
    "u0 = float(u0[0])\n",
    "\n",
    "for cadinality in [30,50]:\n",
    "    print(\"{}:\".format(cadinality))\n",
    "    warnings.filterwarnings('ignore')\n",
    "    assortment_MNL = []\n",
    "    for i in tqdm(range(utility_MNL.shape[0])):\n",
    "        assortment_MNL.append(at.MNL(np.exp(utility_MNL[i,0:100]-u0),price[i,0:100],capacity = cadinality))\n",
    "    products = []\n",
    "    price_MNL = []\n",
    "    for idx,item in enumerate(assortment_MNL):\n",
    "        for j in range(utility_MNL.shape[1]):\n",
    "            if j in item:\n",
    "                products.append(idx*utility_MNL.shape[1]+j)\n",
    "                price_MNL.append(price[idx,j])\n",
    "            elif j >= 100:\n",
    "                products.append(idx*utility_MNL.shape[1]+j)\n",
    "                price_MNL.append(0)\n",
    "    valid_pro = true_model.true_model(next_valid_data.loc[products],1000)\n",
    "    profit = np.dot(valid_pro,np.array(price_MNL))/1000\n",
    "    print(\"profit: {}\".format(profit))\n",
    "    assortment_array = np.zeros((1000,100))\n",
    "    for i,item in enumerate(assortment_MNL):\n",
    "        for j in item:\n",
    "            assortment_array[i,j] = 1\n",
    "    np.save(\"Assortment\\\\MNL_assortment_{}_{}_{}_{}.npy\".format(c_rate,order_num,len(feature_column),cadinality),assortment_array)\n",
    "    a = np.sum(assortment_array,axis = 1)\n",
    "    print(np.min(a),np.mean(a),np.max(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 DeepFM-based Choice Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training the DeepFM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data\n",
    "total = data.shape[1]\n",
    "indices = np.random.permutation(total)\n",
    "k_fold = 5\n",
    "training_data = data[:,indices[0:int(total*(k_fold-1)/k_fold)],:,:]\n",
    "training_label = label[indices[0:int(total*(k_fold-1)/k_fold)],:]\n",
    "training_valid_data = data[:,indices[int(total*(k_fold-1)/k_fold):],:,:]\n",
    "training_valid_label = label[indices[int(total*(k_fold-1)/k_fold):],:]\n",
    "weight = int((training_label.shape[0]*training_label.shape[1]-np.sum(training_label))/np.sum(training_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "PATH = \"trained_models\\\\DeepFM_parameters_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_DeepFM = tp.train_DeepFM(training_data,training_label,training_valid_data,training_valid_label,weight1 = weight,path = PATH,LR = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DeepFM Top1-acc: 0.2566\n",
      " DeepFM Top3-acc: 0.5236\n",
      " Averaged raito of DeepFM: 3.6644734484434127\n",
      "RMSE of DeepFM: 0.02973\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_models\\\\DeepFM_parameters_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_DeepFM = torch.load(PATH)\n",
    "purchase_pro = np.sum(label)/label.shape[0]\n",
    "\n",
    "X = torch.from_numpy(test_data[0][:,:,feature_column]).to(torch.long)\n",
    "weight = torch.from_numpy(test_data[1][:,:,feature_column]).to(torch.float)\n",
    "utility_DeepFM = model_DeepFM([X,weight])\n",
    "pro_DeepFM = torch.sigmoid(utility_DeepFM).detach().numpy()\n",
    "\n",
    "ratio = 0\n",
    "high_low_ratio = []\n",
    "for i in range(pro_DeepFM.shape[0]):\n",
    "    ratio = ratio + (np.max(pro_DeepFM[i])- np.min(pro_DeepFM[i]))/np.mean(pro_DeepFM[i])\n",
    "    # index_array = np.argsort(-valid_price[i])\n",
    "    # high_low_ratio.append(pro_DeepFM[i,index_array[1]]/pro_DeepFM[i,index_array[0]])\n",
    "\n",
    "rmse = []\n",
    "count_number1 = 0\n",
    "count_number3 = 0\n",
    "for i in range(pro_DeepFM.shape[0]):\n",
    "    temp_pro = purchase_pro*pro_DeepFM[i]/np.sum(pro_DeepFM[i])\n",
    "    rmse.append(np.sum((temp_pro - pro[i,0:30])**2) + (np.sum(temp_pro) - np.sum(pro[i,0:30]))**2)\n",
    "    pro_index = np.argsort(-pro_DeepFM[i])\n",
    "    if np.sum(test_label[i,:]) > 0:\n",
    "        if np.argmax(test_label[i,:]) in list(pro_index[0:1]):\n",
    "            count_number1 += 1\n",
    "        if np.argmax(test_label[i,:]) in list(pro_index[0:3]):\n",
    "            count_number3 += 1\n",
    "print(\" DeepFM Top1-acc: {:.4}\".format(count_number1/np.sum(test_label)))\n",
    "print(\" DeepFM Top3-acc: {:.4}\".format(count_number3/np.sum(test_label)))\n",
    "print(\" Averaged raito of DeepFM: {}\".format(ratio/10000))\n",
    "print(\"RMSE of DeepFM: {:.4}\".format(np.sqrt(sum(rmse)/(10000*31))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Assortment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 150.451583129403\n",
      "30 120.29881388711698\n",
      "50 92.92911028833726\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_models\\\\DeepFM_parameters_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_DeepFM = torch.load(PATH)\n",
    "\n",
    "X = torch.from_numpy(valid_data[0]).to(torch.long)\n",
    "weight = torch.from_numpy(valid_data[1]).to(torch.float)\n",
    "utility_DeepFM  = model_DeepFM([X,weight])\n",
    "pro_DeepFM = torch.sigmoid(utility_DeepFM).detach().numpy()\n",
    "\n",
    "for cadinality in [10,30,50]:\n",
    "    warnings.filterwarnings('ignore')\n",
    "    assortment_DeepFM = []\n",
    "    for i in range(pro_DeepFM.shape[0]):\n",
    "        #length = len(assortment_ml[i])\n",
    "        revenue = pro_DeepFM[i,0:100]*price[i,0:100]\n",
    "        sort_index = np.argsort(-revenue)\n",
    "        assortment_DeepFM.append(sort_index[0:cadinality])\n",
    "\n",
    "    products = []\n",
    "    price_DeepFM = []\n",
    "    for idx,item in enumerate(assortment_DeepFM):\n",
    "        for j in range(valid_data.shape[2]):\n",
    "            if j in item:\n",
    "                products.append(idx*valid_data.shape[2]+j)\n",
    "                price_DeepFM.append(price[idx,j])\n",
    "            elif j >= 100:\n",
    "                products.append(idx*valid_data.shape[2]+j)\n",
    "                price_DeepFM.append(0)\n",
    "    valid_pro = true_model.true_model(next_valid_data.loc[products],1000)\n",
    "    average_profit_ml = np.dot(valid_pro,np.array(price_DeepFM))/1000\n",
    "    print(cadinality,average_profit_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 DeepFM-a based choice model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training the DeepFM-a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "total = data_with_assort_info.shape[1]\n",
    "indices = np.random.permutation(total) \n",
    "k_fold = 5\n",
    "training_data = data_with_assort_info[:,indices[0:int(total*(k_fold-1)/k_fold)],:,:]\n",
    "training_label = label[indices[0:int(total*(k_fold-1)/k_fold)],:]\n",
    "training_valid_data = data_with_assort_info[:,indices[int(total*(k_fold-1)/k_fold):],:,:]\n",
    "training_valid_label = label[indices[int(total*(k_fold-1)/k_fold):],:]\n",
    "weight = int((training_label.shape[0]*training_label.shape[1]-np.sum(training_label))/np.sum(training_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "PATH = \"trained_models\\\\DeepFM_parameters_with_assortment_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_DeepFM = tp.train_DeepFM(training_data,training_label,training_valid_data,training_valid_label,model_name = 'Assortment',weight1 = weight,LR = 0.05,path = PATH,feature_column = feature_column_swapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DeepFM-assort Top1-acc: 0.2587\n",
      " DeepFM-assort Top3-acc: 0.5252\n",
      " Averaged raito of DeepFM-assort: 3.6495547036886213\n",
      "RMSE of DeepFM-assort: 0.02974\n"
     ]
    }
   ],
   "source": [
    "PATH = \"trained_models\\\\DeepFM_parameters_with_assortment_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "model_DeepFM = torch.load(PATH)\n",
    "purchase_pro = np.sum(label)/label.shape[0]\n",
    "\n",
    "X = torch.from_numpy(test_data_with_assort_info[0][:,:,feature_column_swapping]).to(torch.long)\n",
    "weight = torch.from_numpy(test_data_with_assort_info[1][:,:,feature_column_swapping]).to(torch.float)\n",
    "utility_DeepFM  = model_DeepFM([X,weight])\n",
    "pro_DeepFM = torch.sigmoid(utility_DeepFM).detach().numpy()\n",
    "\n",
    "ratio = 0\n",
    "high_low_ratio = []\n",
    "for i in range(pro_DeepFM.shape[0]):\n",
    "    ratio = ratio + (np.max(pro_DeepFM[i])- np.min(pro_DeepFM[i]))/np.mean(pro_DeepFM[i])\n",
    "    # index_array = np.argsort(-valid_price[i])\n",
    "    # high_low_ratio.append(pro_DeepFM[i,index_array[1]]/pro_DeepFM[i,index_array[0]])\n",
    "\n",
    "rmse = []\n",
    "count_number1 = 0\n",
    "count_number3 = 0\n",
    "for i in range(pro_DeepFM.shape[0]):\n",
    "    temp_pro = purchase_pro*pro_DeepFM[i]/np.sum(pro_DeepFM[i])\n",
    "    rmse.append(np.sum((temp_pro - pro[i,0:30])**2) + (np.sum(temp_pro) - np.sum(pro[i,0:30]))**2)\n",
    "    pro_index = np.argsort(-pro_DeepFM[i])\n",
    "    if np.sum(test_label[i,:]) > 0:\n",
    "        if np.argmax(test_label[i,:]) in list(pro_index[0:1]):\n",
    "            count_number1 += 1\n",
    "        if np.argmax(test_label[i,:]) in list(pro_index[0:3]):\n",
    "            count_number3 += 1\n",
    "print(\" DeepFM-assort Top1-acc: {:.4}\".format(count_number1/np.sum(test_label)))\n",
    "print(\" DeepFM-assort Top3-acc: {:.4}\".format(count_number3/np.sum(test_label)))\n",
    "print(\" Averaged raito of DeepFM-assort: {}\".format(ratio/10000))\n",
    "print(\"RMSE of DeepFM-assort: {:.4}\".format(np.sqrt(sum(rmse)/(10000*31))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Assortment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [22:36<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 141.82146150912527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:23:51<00:00,  5.03s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 118.30328563586444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [46:49<00:00,  2.81s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 92.61878254975801\n"
     ]
    }
   ],
   "source": [
    "for cadinality in [10,30,50]:\n",
    "    PATH = \"trained_models\\\\DeepFM_parameters_with_assortment_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column))\n",
    "    model_DeepFM = torch.load(PATH)\n",
    "    \n",
    "    model_DeepFM1 = torch.load(\"trained_models\\\\DeepFM_parameters_{}_{}_{}.pt\".format(c_rate,order_num,len(feature_column)))\n",
    "    X = torch.from_numpy(valid_data[0]).to(torch.long)\n",
    "    weight = torch.from_numpy(valid_data[1]).to(torch.float)\n",
    "    utility_DeepFM  = model_DeepFM1([X,weight])\n",
    "    pro_DeepFM = torch.sigmoid(utility_DeepFM).detach().numpy()\n",
    "    assortment_DeepFM = []\n",
    "    for i in range(pro_DeepFM.shape[0]):\n",
    "        #length = len(assortment_ml[i])\n",
    "        revenue = pro_DeepFM[i,0:100]*price[i,0:100]\n",
    "        sort_index = np.argsort(-revenue)\n",
    "        assortment_DeepFM.append(sort_index[0:cadinality])\n",
    "    warnings.filterwarnings('ignore')\n",
    "    assortment_ml = []\n",
    "    for i in tqdm(range(1000)):\n",
    "        assortment_ml.append(at.ml_assortment_swap(model_DeepFM,valid_data[:,i,0:100,:],price[i,0:100],assortment_DeepFM[i],capacity = cadinality,feature_column = feature_column_swapping))\n",
    "    products = []\n",
    "    price_ml = []\n",
    "    for idx,item in enumerate(assortment_ml):\n",
    "        for j in range(valid_data.shape[2]):\n",
    "            if j in item:\n",
    "                products.append(idx*valid_data.shape[2]+j)\n",
    "                price_ml.append(price[idx,j])\n",
    "            elif j >= 100:\n",
    "                products.append(idx*valid_data.shape[2]+j)\n",
    "                price_ml.append(0)\n",
    "    valid_pro = true_model.true_model(next_valid_data.loc[products],1000)\n",
    "    average_profit_ml = np.dot(valid_pro,np.array(price_ml))/1000\n",
    "    print(cadinality,average_profit_ml)\n",
    "    assortment_array = np.zeros((1000,100))\n",
    "    for i,item in enumerate(assortment_ml):\n",
    "        for j in item:\n",
    "            assortment_array[i,j] = 1   \n",
    "    np.save(\"Assortment\\\\DeepFM_assortment_swapping_{}_{}_{}_{}.npy\".format(c_rate,order_num,len(feature_column),cadinality),assortment_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train the transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_rate = 5\n",
    "order_num = 20000\n",
    "train_data = pd.read_csv('train_dataset_for_transformer\\\\{}_{}.csv'.format(c_rate,order_num))\n",
    "weight_index = np.sum(train_data['label'])/(len(list(train_data['orderid'].unique()))-np.sum(train_data['label']))\n",
    "if weight_index > 1:\n",
    "    pt_weight = [1,int(weight_index)]\n",
    "else:\n",
    "    pt_weight = [int(1/weight_index),1]\n",
    "pt_weight = [8,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modifiled_Transformer.train_model(train_data,pt_weight = pt_weight,model_name = \"{}_{}\".format(c_rate,order_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Obtaining the prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_dataset_for_transformer\\\\{}_{}.csv'.format(c_rate,10000))\n",
    "label = np.load('test_dataset_numpy\\\\label_{}_{}.npy'.format(c_rate,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_pred = Modifiled_Transformer.predict(test_data[test_data.columns[0:18]],pt_weight = pt_weight,model_name = \"{}_{}\".format(c_rate,order_num))\n",
    "pro_pred = pro_pred.detach().numpy()\n",
    "total = 0\n",
    "count = 0\n",
    "for i in range(pro_pred.shape[0]):\n",
    "    if np.sum(label[i,:]) == 1:\n",
    "        total = total + 1\n",
    "        if np.argmax(pro_pred[i,:]) == np.argmax(label[i,:]):\n",
    "            count = count + 1\n",
    "print(count/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Get the assortment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepFM1 = torch.load('trained_models\\\\DeepFM_parameters_'+str(c_rate)+'_'+str(order_num )+'.pt')\n",
    "X = torch.from_numpy(valid_data[0]).to(torch.long)\n",
    "weight = torch.from_numpy(valid_data[1]).to(torch.float)\n",
    "utility_DeepFM  = model_DeepFM1([X,weight])\n",
    "pro_DeepFM = torch.sigmoid(utility_DeepFM).detach().numpy()\n",
    "assortment_DeepFM = []\n",
    "for i in range(pro_DeepFM.shape[0]):\n",
    "    #length = len(assortment_ml[i])\n",
    "    revenue = pro_DeepFM[i,0:100]*price[i,0:100]\n",
    "    sort_index = np.argsort(-revenue)\n",
    "    assortment_DeepFM.append(sort_index[0:30])\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#assortment_transformer = []\n",
    "#tasks = []\n",
    "assortment_transformer = np.load(\"Assortment//Transformer_{}_{}.npy\".format(c_rate,order_num),allow_pickle=True)\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    if len(assortment_transformer[i]) != 30:\n",
    "        temp_df = next_valid_data[next_valid_data.orderid == i]\n",
    "        temp_df.reset_index(drop = True,inplace = True)\n",
    "        temp_df = temp_df.loc[0:99,:]\n",
    "        temp_price = np.array(temp_df.loc[0:99,'totalPrice'])\n",
    "        assortment_transformer[i] = at.transformer_assortment_swap(\"{}_{}\".format(c_rate,order_num),temp_df,list(assortment_DeepFM[i]),pt_weight,temp_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4dbc828abb6ce728a8b93da7b8ed7a63770a70f4d8c4b8f352b606b709a27647"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
