{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "import traing_function_real_data as tp\n",
    "import build_new_data as bnd\n",
    "# import assortment as at\n",
    "import warnings\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the original data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('df_OR_normal_process_with_nonpurchase_clean.csv')\n",
    "data = data.drop(columns=['prop_id', 'srch_length_of_stay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(data['prop_location_score1']), min(data['prop_location_score1']))\n",
    "print(max(data['prop_location_score2']), min(data['prop_location_score2']))\n",
    "print(max(data['prop_log_historical_price']), min(data['prop_log_historical_price']))\n",
    "print(max(data['price_usd']), min(data['price_usd']))\n",
    "print(max(data['srch_adults_count']), min(data['srch_adults_count']))\n",
    "print(max(data['srch_children_count']), min(data['srch_children_count']))\n",
    "print(max(data['srch_booking_window']), min(data['srch_booking_window']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conti_columns = ['prop_location_score1', 'prop_location_score2', \n",
    "                    'prop_log_historical_price', 'price_usd', 'srch_adults_count', 'srch_children_count']\n",
    "for column in conti_columns:\n",
    "  max1 = max(data[column])\n",
    "  min1 = min(data[column])\n",
    "  data[column] = (data[column]-min1)/(max1-min1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['prop_starrating', 'prop_review_score', 'prop_brand_bool','promotion_flag','srch_saturday_night_bool','srch_booking_window']\n",
    "feature_sizes = []\n",
    "for cat in cat_columns:\n",
    "    temp_list = list(data.groupby(cat).groups)\n",
    "    temp_dic = {}\n",
    "    for i in range(len(temp_list)):\n",
    "        temp_dic[temp_list[i]] = i\n",
    "    data[cat] = data[cat].transform(lambda s: temp_dic[s])\n",
    "    print(cat, len(temp_list))\n",
    "    feature_sizes.append(len(temp_list))\n",
    "feature_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_order_list = list(set(data['srch_id']))\n",
    "total_num = len(total_order_list)\n",
    "\n",
    "total_dic = {}\n",
    "for rows in range(len(data)):\n",
    "    temp_id = data.loc[rows,'srch_id']\n",
    "    if temp_id in total_dic:\n",
    "        total_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        total_dic[temp_id] = [rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_list_random = np.random.permutation(total_order_list)\n",
    "train_order = order_list_random[:int(0.8*total_num)]\n",
    "valid_order = order_list_random[int(0.8*total_num):int(0.9*total_num)]\n",
    "test_order = order_list_random[int(0.9*total_num):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "valid_list = []\n",
    "test_list = []\n",
    "for orderid in train_order:\n",
    "    train_list.extend(total_dic[orderid])\n",
    "for orderid in valid_order:\n",
    "    valid_list.extend(total_dic[orderid])\n",
    "for orderid in test_order:\n",
    "    test_list.extend(total_dic[orderid])\n",
    "train_data = data.loc[train_list]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "valid_data = data.loc[valid_list]\n",
    "valid_data = valid_data.reset_index(drop=True)\n",
    "test_data = data.loc[test_list]\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the assortment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dic = {}\n",
    "for rows in range(len(train_data)):\n",
    "    temp_id = train_data.loc[rows,'srch_id']\n",
    "    if temp_id in train_dic:\n",
    "        train_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        train_dic[temp_id] = [rows]\n",
    "\n",
    "valid_dic = {}\n",
    "temp_id = -1\n",
    "for rows in range(len(valid_data)):\n",
    "    temp_id = valid_data.loc[rows,'srch_id']\n",
    "    if temp_id in valid_dic:\n",
    "        valid_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        valid_dic[temp_id] = [rows]\n",
    "\n",
    "test_dic = {}\n",
    "temp_id = -1\n",
    "for rows in range(len(test_data)):\n",
    "    temp_id = test_data.loc[rows,'srch_id']\n",
    "    if temp_id in test_dic:\n",
    "        test_dic[temp_id].append(rows)\n",
    "    else:\n",
    "        test_dic[temp_id] = [rows]\n",
    "np.save('train_data_for_simulator\\\\train_dic.npy', train_dic)\n",
    "np.save('train_data_for_simulator\\\\valid_dic.npy', valid_dic)\n",
    "np.save('train_data_for_simulator\\\\test_dic.npy', test_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "label = train_data['booking_bool']\n",
    "del_feature = ['srch_id','booking_bool']\n",
    "features = [i for i in train_data.columns if i not in del_feature]\n",
    "\n",
    "data_np = np.array(train_data[features])\n",
    "data_np_cate = np.zeros(shape = data_np.shape)\n",
    "for idx,item in enumerate([6, 10, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2]):\n",
    "    if item !=1 :\n",
    "        data_np_cate[:,idx] = data_np[:,idx]\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "for idx,item in enumerate([6, 10, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2]):\n",
    "    if item != 1:\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "total_train_data = np.array([data_np_cate,data_np])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_data_for_simulator\\data_ztn.npy',total_train_data)\n",
    "np.save('train_data_for_simulator\\label_ztn.npy',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 105525, 12)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = valid_data['booking_bool']\n",
    "del_feature = ['srch_id','booking_bool']\n",
    "features = [i for i in valid_data.columns if i not in del_feature]\n",
    "data_np = np.array(valid_data[features])\n",
    "data_np_cate = np.zeros(shape = data_np.shape)\n",
    "for idx,item in enumerate([6, 10, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2]):\n",
    "    if item !=1 :\n",
    "        data_np_cate[:,idx] = data_np[:,idx]\n",
    "        data_np[:,idx] = 1\n",
    "        \n",
    "for idx,item in enumerate([6, 10, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2]):\n",
    "    if item != 1:\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "total_valid_data = np.array([data_np_cate,data_np])\n",
    "total_valid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_data_for_simulator\\\\valid_data_ztn.npy',total_valid_data)\n",
    "np.save('train_data_for_simulator\\\\valid_label_ztn.npy',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = test_data['booking_bool']\n",
    "del_feature = ['srch_id','booking_bool']\n",
    "features = [i for i in test_data.columns if i not in del_feature]\n",
    "data_np = np.array(test_data[features])\n",
    "data_np_cate = np.zeros(shape = data_np.shape)\n",
    "for idx,item in enumerate([6, 10, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2]):\n",
    "    if item !=1 :\n",
    "        data_np_cate[:,idx] = data_np[:,idx]\n",
    "        data_np[:,idx] = 1\n",
    "        \n",
    "for idx,item in enumerate([6, 10, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2]):\n",
    "    if item != 1:\n",
    "        data_np[:,idx] = 1\n",
    "\n",
    "total_test_data = np.array([data_np_cate,data_np])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_data_for_simulator\\\\test_data_ztn.npy',total_test_data)\n",
    "np.save('train_data_for_simulator\\\\test_label_ztn.npy',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('train_data_for_simulator\\\\data_ztn.npy')\n",
    "train_label = np.load('train_data_for_simulator\\\\label_ztn.npy')\n",
    "train_dic = np.load('train_data_for_simulator\\\\train_dic.npy', allow_pickle=True).item()\n",
    "\n",
    "valid_data = np.load('train_data_for_simulator\\\\valid_data_ztn.npy')\n",
    "valid_label = np.load('train_data_for_simulator\\\\valid_label_ztn.npy')\n",
    "valid_dic = np.load('train_data_for_simulator\\\\valid_dic.npy', allow_pickle=True).item()\n",
    "\n",
    "test_data = np.load('train_data_for_simulator\\\\test_data_ztn.npy')\n",
    "test_label = np.load('train_data_for_simulator\\\\test_label_ztn.npy')\n",
    "test_dic = np.load('train_data_for_simulator\\\\test_dic.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(train_data)\n",
    "train_data_assort = np.array([np.concatenate([data[0],np.zeros((841500, 6)),np.zeros((841500, 4))],axis = 1),np.concatenate([data[1],np.ones((841500, 6)),np.ones((841500, 4))],axis = 1)])\n",
    "for i in train_dic.values():\n",
    "    data_temp = np.zeros((len(i), 6))\n",
    "    data_cate_temp = np.zeros((len(i), 4))\n",
    "    min1 = np.min(data[1,i,5])\n",
    "    max1 = np.max(data[1,i,5])\n",
    "    mean1 = np.mean(data[1,i,5])\n",
    "    min2 = np.min(data[1,i,6])\n",
    "    max2 = np.max(data[1,i,6])\n",
    "    mean2 = np.mean(data[1,i,6])\n",
    "    data_temp[:,0] = min1\n",
    "    data_temp[:,1] = max1\n",
    "    data_temp[:,2] = mean1\n",
    "    data_temp[:,3] = min2\n",
    "    data_temp[:,4] = max2\n",
    "    data_temp[:,5] = mean2\n",
    "    for idx,j in enumerate(i):\n",
    "        if data[1,j,5] == np.min(data[1,i,5]):\n",
    "            data_cate_temp[idx,0] = 1\n",
    "        if data[1,j,5] == np.max(data[1,i,5]):\n",
    "            data_cate_temp[idx,1] = 1\n",
    "        \n",
    "        if data[1,j,6] == np.min(data[1,i,6]):\n",
    "            data_cate_temp[idx,2] = 1\n",
    "        if data[1,j,6] == np.max(data[1,i,6]):\n",
    "            data_cate_temp[idx,3] = 1\n",
    "    train_data_assort[0,i,18:] = data_cate_temp\n",
    "    train_data_assort[1,i,12:18] = data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(valid_data)\n",
    "valid_data_assort = np.array([np.concatenate([data[0],np.zeros((data.shape[1], 6)),np.zeros((data.shape[1], 4))],axis = 1),np.concatenate([data[1],np.ones((data.shape[1], 6)),np.ones((data.shape[1], 4))],axis = 1)])\n",
    "for i in valid_dic.values():\n",
    "    data_temp = np.zeros((len(i), 6))\n",
    "    data_cate_temp = np.zeros((len(i), 4))\n",
    "    min1 = np.min(data[1,i,5])\n",
    "    max1 = np.max(data[1,i,5])\n",
    "    mean1 = np.mean(data[1,i,5])\n",
    "    min2 = np.min(data[1,i,6])\n",
    "    max2 = np.max(data[1,i,6])\n",
    "    mean2 = np.mean(data[1,i,6])\n",
    "    data_temp[:,0] = min1\n",
    "    data_temp[:,1] = max1\n",
    "    data_temp[:,2] = mean1\n",
    "    data_temp[:,3] = min2\n",
    "    data_temp[:,4] = max2\n",
    "    data_temp[:,5] = mean2\n",
    "    for idx,j in enumerate(i):\n",
    "        if data[1,j,5] == np.min(data[1,i,5]):\n",
    "            data_cate_temp[idx,0] = 1\n",
    "        if data[1,j,5] == np.max(data[1,i,5]):\n",
    "            data_cate_temp[idx,1] = 1\n",
    "        \n",
    "        if data[1,j,6] == np.min(data[1,i,6]):\n",
    "            data_cate_temp[idx,2] = 1\n",
    "        if data[1,j,6] == np.max(data[1,i,6]):\n",
    "            data_cate_temp[idx,3] = 1\n",
    "    valid_data_assort[0,i,18:] = data_cate_temp\n",
    "    valid_data_assort[1,i,12:18] = data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(test_data)\n",
    "test_data_assort = np.array([np.concatenate([data[0],np.zeros((data.shape[1], 6)),np.zeros((data.shape[1], 4))],axis = 1),np.concatenate([data[1],np.ones((data.shape[1], 6)),np.ones((data.shape[1], 4))],axis = 1)])\n",
    "for i in test_dic.values():\n",
    "    data_temp = np.zeros((len(i), 6))\n",
    "    data_cate_temp = np.zeros((len(i), 4))\n",
    "    min1 = np.min(data[1,i,5])\n",
    "    max1 = np.max(data[1,i,5])\n",
    "    mean1 = np.mean(data[1,i,5])\n",
    "    min2 = np.min(data[1,i,6])\n",
    "    max2 = np.max(data[1,i,6])\n",
    "    mean2 = np.mean(data[1,i,6])\n",
    "    data_temp[:,0] = min1\n",
    "    data_temp[:,1] = max1\n",
    "    data_temp[:,2] = mean1\n",
    "    data_temp[:,3] = min2\n",
    "    data_temp[:,4] = max2\n",
    "    data_temp[:,5] = mean2\n",
    "    for idx,j in enumerate(i):\n",
    "        if data[1,j,5] == np.min(data[1,i,5]):\n",
    "            data_cate_temp[idx,0] = 1\n",
    "        if data[1,j,5] == np.max(data[1,i,5]):\n",
    "            data_cate_temp[idx,1] = 1\n",
    "        \n",
    "        if data[1,j,6] == np.min(data[1,i,6]):\n",
    "            data_cate_temp[idx,2] = 1\n",
    "        if data[1,j,6] == np.max(data[1,i,6]):\n",
    "            data_cate_temp[idx,3] = 1\n",
    "    test_data_assort[0,i,18:] = data_cate_temp\n",
    "    test_data_assort[1,i,12:18] = data_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MNL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Traing the MNL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_MNL = np.concatenate((train_data,valid_data),axis = 1)\n",
    "train_label_MNL = np.concatenate((train_label,valid_label),axis = 0)\n",
    "train_dic_MNL = deepcopy(train_dic)\n",
    "for i in valid_dic.keys():\n",
    "    if i in train_dic.keys():\n",
    "        print('error')\n",
    "    else:\n",
    "        train_dic_MNL[i] = list(np.array(valid_dic[i]) + train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_model_ztn\\\\MNL_parameters_ztn.pt\"\n",
    "model_MNL = tp.train_data(train_data_MNL,train_label_MNL, train_dic_MNL, NUM_EPOCHS = 100,BATCH_SIZE = 32,path = PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MNL = torch.load(\"trained_model_ztn\\\\MNL_parameters_ztn.pt\")\n",
    "X = torch.from_numpy(test_data[0]).to(torch.long)\n",
    "weight = torch.from_numpy(test_data[1]).to(torch.float)\n",
    "utility_MNL = model_MNL([X,weight])\n",
    "utility_MNL = utility_MNL.detach().numpy()\n",
    "#u = float(u.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in test_dic:\n",
    "    temp_lst = test_dic[item]\n",
    "    temp_sum = np.sum(np.exp(np.array(utility_MNL[test_dic[item]]))) \n",
    "    for i in temp_lst:\n",
    "        utility_MNL[i] = np.exp(utility_MNL[i]) /temp_sum\n",
    "\n",
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    rmse.append(np.sum((utility_MNL[test_dic[item]]-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-utility_MNL[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 MMNL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_MNL = np.concatenate((train_data,valid_data),axis = 1)\n",
    "train_label_MNL = np.concatenate((train_label,valid_label),axis = 0)\n",
    "train_dic_MNL = deepcopy(train_dic)\n",
    "for i in valid_dic.keys():\n",
    "    if i in train_dic.keys():\n",
    "        print('error')\n",
    "    else:\n",
    "        train_dic_MNL[i] = list(np.array(valid_dic[i]) + train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH = \"trained_model_ztn\\\\MMNL_parameters_ztn.pt\"\n",
    "model_MMNL_list,alpha = tp.CG_algo(train_data_MNL,train_label_MNL, train_dic_MNL, NUM_EPOCHS = 100,BATCH_SIZE = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MMNL_list = np.load(\"trained_model_ztn\\\\MMNL.npy\",allow_pickle=True)\n",
    "alpha = np.load(\"trained_model_ztn\\\\alpha.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_temp = []\n",
    "for model_MNL in model_MMNL_list:\n",
    "    X = torch.from_numpy(test_data[0]).to(torch.long)\n",
    "    weight = torch.from_numpy(test_data[1]).to(torch.float)\n",
    "    utility_MNL = model_MNL([X,weight])\n",
    "    utility_MNL = utility_MNL.detach().numpy()\n",
    "    for item in test_dic:\n",
    "        temp_lst = test_dic[item]\n",
    "        temp_sum = np.sum(np.exp(np.array(utility_MNL[test_dic[item]]))) \n",
    "        for i in temp_lst:\n",
    "            utility_MNL[i] = np.exp(utility_MNL[i]) /temp_sum\n",
    "    pro_temp.append(utility_MNL)\n",
    "pro = 0\n",
    "for i in range(len(pro_temp)):\n",
    "    pro = alpha[i]*np.array(pro_temp[i]) + pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.19198633144423044\n",
      "0.29910821884791516\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    rmse.append(np.sum((pro[test_dic[item]]-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-pro[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_model_ztn\\\\DeepFM_parameters_ztn.pt\"\n",
    "model_DeepFM = tp.train_DeepFM(train_data,train_label,valid_data,valid_label,valid_dic, NUM_EPOCHS = 100,BATCH_SIZE = 256,LR = 0.05,weight1 = 31,path = PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepFM = torch.load(\"trained_model_ztn\\\\DeepFM_parameters_ztn.pt\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cat = torch.from_numpy(test_data[0]).to(torch.long).to(device)\n",
    "weight = torch.from_numpy(test_data[1]).to(torch.float).to(device)\n",
    "outputs = model_DeepFM([cat, weight])\n",
    "outputs = outputs.cpu().detach().numpy()\n",
    "pro = np.exp(outputs)/(np.exp(outputs)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.1922712930116513\n",
      "0.35936370209689084\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    temp_pro = pro[test_dic[item]]/np.sum(pro[test_dic[item]])\n",
    "    rmse.append(np.sum((temp_pro-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-pro[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 DeepFM-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_model_ztn\\\\DeepFM-a_parameters_ztn.pt\"\n",
    "model_DeepFM = tp.train_DeepFMa(train_data_assort,train_label,valid_data_assort,valid_label,valid_dic, NUM_EPOCHS = 100,BATCH_SIZE = 256,LR = 0.05,weight1 = 31,path = PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DeepFM = torch.load(\"trained_model_ztn\\\\DeepFM-a_parameters_ztn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cat = torch.from_numpy(test_data_assort[0]).to(torch.long).to(device)\n",
    "weight = torch.from_numpy(test_data_assort[1]).to(torch.float).to(device)\n",
    "outputs = model_DeepFM([cat, weight])\n",
    "outputs = outputs.cpu().detach().numpy()\n",
    "pro = np.exp(outputs)/(np.exp(outputs)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.19214619268848493\n",
      "0.34683056158110387\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    temp_pro = pro[test_dic[item]]/np.sum(pro[test_dic[item]])\n",
    "    rmse.append(np.sum((temp_pro-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-pro[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Exponential-Based 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_exp = np.concatenate((train_data,valid_data),axis = 1)\n",
    "train_label_exp = np.concatenate((train_label,valid_label),axis = 0)\n",
    "train_dic_exp = deepcopy(train_dic)\n",
    "for i in valid_dic.keys():\n",
    "    if i in train_dic.keys():\n",
    "        print('error')\n",
    "    else:\n",
    "        train_dic_exp[i] = list(np.array(valid_dic[i]) + train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_model_ztn\\\\EXP_parameters_ztn.pt\"\n",
    "model_MNL = tp.train_data(train_data_exp,train_label_exp, train_dic_exp, NUM_EPOCHS = 100,BATCH_SIZE = 32,path = PATH, model_type = 'EXP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"trained_model_ztn\\\\EXP_parameters_ztn.pt\"\n",
    "model_exp = torch.load(PATH)\n",
    "X = torch.from_numpy(test_data[0]).to(torch.long)\n",
    "weight = torch.from_numpy(test_data[1]).to(torch.float)\n",
    "utility_exp = model_exp([X,weight])\n",
    "utility_exp = utility_exp.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_exp = np.zeros_like(utility_exp)\n",
    "for item in test_dic:\n",
    "    temp_lst = test_dic[item]\n",
    "    utility_temp = utility_exp[test_dic[item]]\n",
    "    utility_temp = np.sort(utility_temp)\n",
    "    for i in temp_lst:\n",
    "        index = np.where(utility_temp == utility_exp[i])[0][0]\n",
    "        Q1 = np.exp(-np.sum(utility_temp[index:] - utility_temp[index]))/(utility_temp.shape[0]-index)\n",
    "        if index == 0:\n",
    "            pro_exp[i] = Q1\n",
    "        else:\n",
    "            Q2 = 0\n",
    "            for j in range(index):\n",
    "                Q2 += np.exp(-np.sum(utility_temp[j:] - utility_temp[j]))/((utility_temp.shape[0]-j-1)*(utility_temp.shape[0]-j))\n",
    "            pro_exp[i] = Q1-Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.19186398916432948\n",
      "0.3234514340805013\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "count = 0\n",
    "top_n = 3\n",
    "rmse = []\n",
    "denominator = 0\n",
    "for item in test_dic:\n",
    "    denominator += len(test_dic[item])\n",
    "    rmse.append(np.sum((pro_exp[test_dic[item]]-test_label[test_dic[item]])**2))\n",
    "    temp_prob_max = np.argsort(-pro_exp[test_dic[item]])[0:top_n]\n",
    "    temp_test_y = list(test_label[test_dic[item]])\n",
    "    total += 1\n",
    "    temp_y_max = temp_test_y.index(max(temp_test_y))\n",
    "    if temp_y_max in temp_prob_max:\n",
    "        count += 1\n",
    "print(\"RMSE: \",np.sqrt(sum(rmse)/denominator))\n",
    "print(count/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dbc828abb6ce728a8b93da7b8ed7a63770a70f4d8c4b8f352b606b709a27647"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
